{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a99cf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction file:  Table_5_1.csv\n",
      "\n",
      "min_sup:  0.001\n",
      "\n",
      "min_conf:  0.8\n",
      "\n",
      "Number of transactions:  5\n",
      "\n",
      "Candidate 1-itemset has 6 items.\n",
      "{'Bread': 4, 'Milk': 4, 'Diapers': 4, 'Beer': 3, 'Eggs': 1, 'Cola': 2}\n",
      "\n",
      "Pruned 1-itemset is the same as the eliminated 1-itemset.\n",
      "\n",
      "Eliminated 1-itemset has 6 items\n",
      "{'Bread': 4, 'Milk': 4, 'Diapers': 4, 'Beer': 3, 'Eggs': 1, 'Cola': 2}\n",
      "\n",
      "Candidate 2-itemset has 15 items.\n",
      "[{'Milk', 'Bread'}, {'Diapers', 'Bread'}, {'Bread', 'Beer'}, {'Eggs', 'Bread'}, {'Cola', 'Bread'}, {'Milk', 'Diapers'}, {'Milk', 'Beer'}, {'Eggs', 'Milk'}, {'Cola', 'Milk'}, {'Diapers', 'Beer'}, {'Eggs', 'Diapers'}, {'Cola', 'Diapers'}, {'Eggs', 'Beer'}, {'Cola', 'Beer'}, {'Cola', 'Eggs'}]\n",
      "\n",
      "Pruned 2-itemset has 15 items.\n",
      "[{'Milk', 'Bread'}, {'Diapers', 'Bread'}, {'Bread', 'Beer'}, {'Eggs', 'Bread'}, {'Cola', 'Bread'}, {'Milk', 'Diapers'}, {'Milk', 'Beer'}, {'Eggs', 'Milk'}, {'Cola', 'Milk'}, {'Diapers', 'Beer'}, {'Eggs', 'Diapers'}, {'Cola', 'Diapers'}, {'Eggs', 'Beer'}, {'Cola', 'Beer'}, {'Cola', 'Eggs'}]\n",
      "\n",
      "Eliminated 2-itemset has 13 items.\n",
      "{'Milk,Bread': 3, 'Diapers,Bread': 3, 'Bread,Beer': 2, 'Eggs,Bread': 1, 'Diapers,Beer': 3, 'Eggs,Diapers': 1, 'Eggs,Beer': 1, 'Milk,Diapers': 3, 'Milk,Beer': 2, 'Cola,Milk': 2, 'Cola,Diapers': 2, 'Cola,Beer': 1, 'Cola,Bread': 1}\n",
      "\n",
      "Candidate 3-itemset has 19 items.\n",
      "[{'Diapers', 'Milk', 'Bread'}, {'Milk', 'Bread', 'Beer'}, {'Eggs', 'Milk', 'Bread'}, {'Cola', 'Milk', 'Bread'}, {'Bread', 'Diapers', 'Beer'}, {'Eggs', 'Bread', 'Diapers'}, {'Cola', 'Bread', 'Diapers'}, {'Eggs', 'Bread', 'Beer'}, {'Cola', 'Bread', 'Beer'}, {'Cola', 'Eggs', 'Bread'}, {'Eggs', 'Diapers', 'Beer'}, {'Milk', 'Diapers', 'Beer'}, {'Cola', 'Diapers', 'Beer'}, {'Eggs', 'Milk', 'Diapers'}, {'Cola', 'Eggs', 'Diapers'}, {'Eggs', 'Milk', 'Beer'}, {'Cola', 'Eggs', 'Beer'}, {'Cola', 'Milk', 'Diapers'}, {'Cola', 'Milk', 'Beer'}]\n",
      "\n",
      "Pruned 3-itemset has 13 items.\n",
      "[{'Diapers', 'Milk', 'Bread'}, {'Milk', 'Bread', 'Beer'}, {'Cola', 'Milk', 'Bread'}, {'Bread', 'Diapers', 'Beer'}, {'Eggs', 'Bread', 'Diapers'}, {'Cola', 'Bread', 'Diapers'}, {'Eggs', 'Bread', 'Beer'}, {'Cola', 'Bread', 'Beer'}, {'Eggs', 'Diapers', 'Beer'}, {'Milk', 'Diapers', 'Beer'}, {'Cola', 'Diapers', 'Beer'}, {'Cola', 'Milk', 'Diapers'}, {'Cola', 'Milk', 'Beer'}]\n",
      "\n",
      "Eliminated 3-itemset has 12 items.\n",
      "{'Bread,Diapers,Beer': 2, 'Eggs,Bread,Diapers': 1, 'Eggs,Bread,Beer': 1, 'Eggs,Diapers,Beer': 1, 'Milk,Diapers,Beer': 2, 'Cola,Diapers,Beer': 1, 'Cola,Milk,Diapers': 2, 'Cola,Milk,Beer': 1, 'Diapers,Milk,Bread': 2, 'Milk,Bread,Beer': 1, 'Cola,Milk,Bread': 1, 'Cola,Bread,Diapers': 1}\n",
      "\n",
      "Candidate 4-itemset has 11 items.\n",
      "[{'Bread', 'Eggs', 'Diapers', 'Beer'}, {'Bread', 'Milk', 'Diapers', 'Beer'}, {'Bread', 'Cola', 'Diapers', 'Beer'}, {'Bread', 'Milk', 'Eggs', 'Diapers'}, {'Bread', 'Cola', 'Eggs', 'Diapers'}, {'Milk', 'Eggs', 'Bread', 'Beer'}, {'Milk', 'Eggs', 'Diapers', 'Beer'}, {'Cola', 'Eggs', 'Diapers', 'Beer'}, {'Milk', 'Cola', 'Diapers', 'Beer'}, {'Bread', 'Milk', 'Cola', 'Diapers'}, {'Milk', 'Cola', 'Bread', 'Beer'}]\n",
      "\n",
      "Pruned 4-itemset has 4 items.\n",
      "[{'Bread', 'Eggs', 'Diapers', 'Beer'}, {'Bread', 'Milk', 'Diapers', 'Beer'}, {'Milk', 'Cola', 'Diapers', 'Beer'}, {'Bread', 'Milk', 'Cola', 'Diapers'}]\n",
      "\n",
      "Eliminated 4-itemset has 4 items.\n",
      "{'Bread,Eggs,Diapers,Beer': 1, 'Milk,Cola,Diapers,Beer': 1, 'Bread,Milk,Diapers,Beer': 1, 'Bread,Milk,Cola,Diapers': 1}\n",
      "\n",
      "Candidate 5-itemset has 2 items.\n",
      "[{'Eggs', 'Diapers', 'Milk', 'Bread', 'Beer'}, {'Diapers', 'Milk', 'Cola', 'Bread', 'Beer'}]\n",
      "\n",
      "Pruned 5-itemset has 0 items.\n",
      "[]\n",
      "\n",
      "Eliminated 5-itemset has 0 items.\n",
      "{}\n",
      "END\n",
      "\n",
      "{Eggs} --> {Bread}   confidence: 1.000\n",
      "\n",
      "{Beer} --> {Diapers}   confidence: 1.000\n",
      "\n",
      "{Eggs} --> {Diapers}   confidence: 1.000\n",
      "\n",
      "{Eggs} --> {Beer}   confidence: 1.000\n",
      "\n",
      "{Cola} --> {Milk}   confidence: 1.000\n",
      "\n",
      "{Cola} --> {Diapers}   confidence: 1.000\n",
      "\n",
      "{Eggs} --> {Bread,Diapers}   confidence: 1.000\n",
      "\n",
      "{Eggs,Bread} --> {Diapers}   confidence: 1.000\n",
      "\n",
      "{Eggs} --> {Bread,Beer}   confidence: 1.000\n",
      "\n",
      "{Eggs,Bread} --> {Beer}   confidence: 1.000\n",
      "\n",
      "{Eggs} --> {Diapers,Beer}   confidence: 1.000\n",
      "\n",
      "{Eggs,Diapers} --> {Beer}   confidence: 1.000\n",
      "\n",
      "{Cola} --> {Milk,Diapers}   confidence: 1.000\n",
      "\n",
      "{Cola,Milk} --> {Diapers}   confidence: 1.000\n",
      "\n",
      "{Cola,Bread} --> {Diapers}   confidence: 1.000\n",
      "\n",
      "{Bread,Eggs} --> {Diapers,Beer}   confidence: 1.000\n",
      "\n",
      "{Eggs} --> {Bread,Diapers,Beer}   confidence: 1.000\n",
      "\n",
      "{Bread,Eggs,Diapers} --> {Beer}   confidence: 1.000\n",
      "\n",
      "{Eggs,Diapers} --> {Bread,Beer}   confidence: 1.000\n",
      "\n",
      "{Eggs,Diapers,Beer} --> {Bread}   confidence: 1.000\n",
      "\n",
      "{Cola,Diapers,Beer} --> {Milk}   confidence: 1.000\n",
      "\n",
      "{Bread,Milk,Cola} --> {Diapers}   confidence: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    import csv\n",
    "    # read data\n",
    "    data_name = 'Table_5_1.csv'\n",
    "    print(\"Transaction file: \", data_name)\n",
    "    file = open(data_name, 'r')\n",
    "    my_reader = csv.reader(file)\n",
    "    data = []\n",
    "    for row in my_reader:\n",
    "        data.append(row)\n",
    "    \n",
    "    # data cleansing\n",
    "    for transaction in data:\n",
    "        transaction = transaction.pop(0)\n",
    "    \n",
    "    # Set hyperparameter (min_sup)\n",
    "    # I chose the common pair of min_sup and min_conf for market basket analysis\n",
    "    MIN_SUP = 0.001\n",
    "    MIN_CONF = 0.8\n",
    "    print(\"\\nmin_sup: \",MIN_SUP)\n",
    "    print(\"\\nmin_conf: \", MIN_CONF)\n",
    "    \n",
    "    # print out the number of transactions\n",
    "    print(\"\\nNumber of transactions: \", len(data))\n",
    "    \n",
    "    # create candidate 1-itemset (initialization)\n",
    "    def create_C1(data):\n",
    "        C1 = {}\n",
    "        for transaction in data:\n",
    "            for item in transaction:\n",
    "                if item in C1:\n",
    "                    C1[item] += 1\n",
    "                else:\n",
    "                    C1[item] = 1\n",
    "        return C1 \n",
    "    \n",
    "    # function for generating candidates of larger itemsets\n",
    "    def candidate_generation(Lk, k, data): \n",
    "        Lk = list(Lk.keys())\n",
    "        for i in range(len(Lk)):\n",
    "            Lk[i] = set(list(Lk[i].split(',')))\n",
    "        Ck = []\n",
    "        for i in range(len(Lk)):\n",
    "            Lk_list = list(Lk[i])\n",
    "            for p in range(len(Lk_list)):\n",
    "                testing_subset = set()\n",
    "                for q in range(len(Lk_list)):\n",
    "                    if q != p:\n",
    "                        testing_subset.add(Lk_list[q])\n",
    "                for j in range(i+1, len(Lk)):# compare each k-itemsets with every other k-itemsets to generate k+1-itemsets with join (disregard infrequent itemsets)\n",
    "                    if testing_subset.issubset(Lk[j]):\n",
    "                        Ck.append(Lk[i] | Lk[j]) # join the two itemsets\n",
    "        Ck_no_duplicate = []\n",
    "        for itemset in Ck:\n",
    "            if itemset not in Ck_no_duplicate:\n",
    "                Ck_no_duplicate.append(itemset)\n",
    "        return Ck_no_duplicate\n",
    "    \n",
    "    # function for pre-pruning candidate itemsets (prune itemsets in Ck with subsets of length k-1 that are infrequent)\n",
    "    def candidate_pruning(prevLk, Ck):\n",
    "        Ck_pruned = []\n",
    "        # convert prevLk to list of lists form\n",
    "        prevLk_list = list(prevLk.keys())\n",
    "        for i in range(len(prevLk_list)):\n",
    "            prevLk_list[i] = list(prevLk_list[i].split(','))\n",
    "        # consider the subsets of each candidate in Ck\n",
    "        for itemset in Ck:\n",
    "            curr_item_list = list(itemset)\n",
    "            # leave one element out and generate subsets of length k-1\n",
    "            for i in range(len(curr_item_list)):\n",
    "                testing_subset = []\n",
    "                for j in range(len(curr_item_list)):\n",
    "                    if j != i:\n",
    "                        testing_subset.append(curr_item_list[j])\n",
    "                testing_subset.sort()\n",
    "                # if a subset is not within the k-1-itemset list, prune the full itemset\n",
    "                flag = 0 # flag for determining whether to prune the candidate itemset\n",
    "                for item in prevLk_list:\n",
    "                    item.sort()\n",
    "                    if testing_subset == item:\n",
    "                        flag = 1 # subset in the prevLk\n",
    "                        break\n",
    "                if flag == 0: # subset not in the prevLk\n",
    "                    break # current candidate pruned, consider the next candidate\n",
    "                if i == len(curr_item_list) - 1:\n",
    "                    Ck_pruned.append(itemset) # if all subsets are within the prevLk, we keep the candidate\n",
    "        return Ck_pruned\n",
    "    \n",
    "    # function for generating the support of each candidate sets\n",
    "    def generate_sup_count_list(data, Ck_pruned):\n",
    "        # convert data to list of set form\n",
    "        data_set = [set() for i in range(len(data))]\n",
    "        for i in range(len(data)):\n",
    "            for item in data[i]:\n",
    "                data_set[i].add(item) \n",
    "        # only for first iteration to see the change more clearer\n",
    "        if type(Ck_pruned) == dict:\n",
    "            Ck_list = list(Ck_pruned.keys())\n",
    "            Ck_pruned = [set() for i in range(len(Ck_list))]\n",
    "            for i in range(len(Ck_pruned)):\n",
    "                Ck_pruned[i].add(Ck_list[i])\n",
    "        sup_count = {}\n",
    "        for transaction in data:\n",
    "            for candidate in Ck_pruned:\n",
    "                if candidate.issubset(transaction):\n",
    "                    candidate = ','.join(str(item) for item in candidate) # convert set to string to search in the dictionary or add a new key in dictionary\n",
    "                    if candidate in sup_count:\n",
    "                        sup_count[candidate] += 1\n",
    "                    else:\n",
    "                        sup_count[candidate] = 1\n",
    "        return sup_count\n",
    "    \n",
    "    # function for generating frequent itemsets (transformation from Ck to Lk)\n",
    "    def candidate_elimination(min_sup, sup_count):\n",
    "        Lk = {}\n",
    "        for key in sup_count:\n",
    "            if sup_count[key] >= min_sup:\n",
    "                Lk[key] = sup_count[key]\n",
    "        return Lk\n",
    "    \n",
    "    # main function for a priori algorithm\n",
    "    def apriori(data, min_sup):\n",
    "        C1 = create_C1(data) # initialization\n",
    "        print(\"\\nCandidate %d-itemset has %d items.\" % (1, len(C1)))\n",
    "        print(C1)\n",
    "        print(\"\\nPruned %d-itemset is the same as the eliminated %d-itemset.\" % (1, 1))\n",
    "        sup_dic = generate_sup_count_list(data, C1) # only support generation and candidate elimination is required for C1\n",
    "        L1 = candidate_elimination(MIN_SUP, sup_dic)\n",
    "        print(\"\\nEliminated %d-itemset has %d items\" % (1,len(L1)))\n",
    "        print(L1)\n",
    "        L = [L1] # create a list of filtered itemsets so we can track the previous sets for arguments such as prevLk\n",
    "        k = 2 # now, we want to generate 2-itemset\n",
    "        \n",
    "        # run loop to continuously generate Ck and filter to Lk until empty set is reached\n",
    "        while len(L[k-2]) > 0:\n",
    "            Ck = candidate_generation(L[k-2], k, data) # candidate generation\n",
    "            print(\"\\nCandidate %d-itemset has %d items.\" % (k, len(Ck)))\n",
    "            print(Ck)\n",
    "            Ck_pruned = candidate_pruning(L[k-2], Ck) # candidate pruning\n",
    "            print(\"\\nPruned %d-itemset has %d items.\" % (k,len(Ck_pruned)))\n",
    "            print(Ck_pruned)\n",
    "            sup_dic = generate_sup_count_list(data, Ck_pruned) # support count dictionary generation\n",
    "            Lk = candidate_elimination(MIN_SUP, sup_dic) # candidate elimination\n",
    "            frequent_after_elimination.append(Lk)\n",
    "            print(\"\\nEliminated %d-itemset has %d items.\" % (k,len(Lk)))\n",
    "            print(Lk)\n",
    "            L.append(Lk)\n",
    "            k += 1\n",
    "        print(\"END\\n\")\n",
    "        return \n",
    "    \n",
    "    ################################################## Association Rules Generation ########################################################\n",
    "    # function for calculating the confidence of two itemsets\n",
    "    def get_confidence(rule, frequent_sets, key_list, i):\n",
    "        joined_list = rule[0] + rule[1]\n",
    "        correct_variation = get_correct_string_variation(joined_list, frequent_sets)\n",
    "        sup_joined_list = frequent_sets[i].get(correct_variation)\n",
    "        correct_variation_LHS = get_correct_string_variation(rule[0], frequent_sets)\n",
    "        sup_rule_LHS = frequent_sets[len(rule[0]) - 1].get(correct_variation_LHS)\n",
    "        confidence = sup_joined_list / sup_rule_LHS\n",
    "        return confidence\n",
    "    \n",
    "    # generate rules\n",
    "    # frequent_sets is a list of dictionaries. Each dictionaries contain k-frequent sets generated from the Apriori algorithm\n",
    "    from itertools import chain, combinations\n",
    "    def rule_generation(frequent_sets, min_confidence):\n",
    "        for i in range(1,len(frequent_sets)):\n",
    "            key_list = list(frequent_sets[i])\n",
    "            for cand_string in key_list:\n",
    "                cand_list = string_to_list(cand_string)\n",
    "                sub_lists = get_sub_lists(cand_list)\n",
    "                candidate_rules = []\n",
    "                for sub_list in sub_lists:\n",
    "                    candidate_rules.append([sub_list,difference_between_two_lists(cand_list,sub_list)]) # each element of this list is a list that contain the associated pair of itemsets\n",
    "                for rule in candidate_rules:\n",
    "                    if len(rule[0]) != 0 and len(rule[1]) != 0 and get_confidence(rule, frequent_sets,key_list, i) >= MIN_CONF:\n",
    "                        my_separator = \",\"\n",
    "                        LHS = my_separator.join(rule[0])\n",
    "                        RHS = my_separator.join(rule[1])\n",
    "                        confidence = \"{:.3f}\".format(get_confidence(rule, frequent_sets, key_list, i))\n",
    "                        print(\"{\", LHS , \"} --> {\", RHS, \"}\", \"   confidence: \", confidence, \"\\n\", sep = \"\")\n",
    "        return\n",
    "                \n",
    "    # definitions of some essential functions that are used within the main functions            \n",
    "    def string_to_list(string):\n",
    "        li = list(string.split(\",\"))\n",
    "        return li\n",
    "    \n",
    "    def get_sub_lists (full_list):\n",
    "        lists = [[]]\n",
    "        for i in range(len(full_list) + 1):\n",
    "            for j in range(i):\n",
    "                lists.append(full_list[j: i])\n",
    "        return lists\n",
    "    \n",
    "    def difference_between_two_lists(list_1, list_2):\n",
    "        return list(set(list_1) - set(list_2)) + list(set(list_2) - set(list_1))\n",
    "    \n",
    "    # choose the correctly ordered string from all the permutations in order to find the support in the frequent set dictionary\n",
    "    from itertools import permutations\n",
    "    def get_correct_string_variation(cand_list, frequent_sets):\n",
    "        my_separator = ','\n",
    "        cand_string = my_separator.join(cand_list)\n",
    "        if frequent_sets[len(cand_list) - 1].get(cand_string, \"NO_KEY\") != \"NO_KEY\":\n",
    "            return cand_string\n",
    "        else:\n",
    "            all_permutations = list(permutations(cand_list, len(cand_list)))\n",
    "            for permutation in all_permutations:\n",
    "                cand_permutation = my_separator.join(list(permutation))\n",
    "                if frequent_sets[len(list(permutation)) - 1].get(cand_permutation, \"NO_KEY\") != \"NO_KEY\":\n",
    "                    return cand_permutation\n",
    "        \n",
    "        \n",
    "                \n",
    "    # Driver Code\n",
    "    frequent_after_elimination = []\n",
    "    frequent_after_elimination.append(create_C1(data))\n",
    "    apriori(data, MIN_SUP)\n",
    "    rule_generation(frequent_after_elimination, MIN_CONF)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392b597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
